import streamlit as st
import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
import plotly.graph_objects as go
from gensim.models import Word2Vec
from transformers import AutoTokenizer, AutoModel
import torch
from scipy.spatial.distance import cosine
import warnings
warnings.filterwarnings('ignore')

# C·∫•u h√¨nh trang
st.set_page_config(page_title="Vietnamese Synonym Analysis", layout="wide")
st.title("üî§ Ph√¢n T√≠ch T·ª´ ƒê·ªìng Nghƒ©a & Tr√°i Nghƒ©a Ti·∫øng Vi·ªát")
st.markdown("### S·ª≠ d·ª•ng GloVe v√† BERT")

# Load data
@st.cache_data
def load_data():
    df = pd.read_csv('Vietnamese_SynAnt.csv')
    return df

# Train GloVe-like model (Word2Vec with similar approach)
@st.cache_resource
def train_glove_model(df):
    # T·∫°o corpus t·ª´ c√°c c·∫∑p t·ª´
    sentences = []
    for _, row in df.iterrows():
        word1, word2, label = row['word1'], row['word2'], row['label']
        sentences.append([word1, word2])
        # Th√™m c·∫£ chi·ªÅu ng∆∞·ª£c l·∫°i
        sentences.append([word2, word1])
    
    # Train Word2Vec model (t∆∞∆°ng t·ª± GloVe)
    model = Word2Vec(sentences=sentences, vector_size=100, window=2, 
                     min_count=1, workers=4, epochs=50, sg=0)
    return model

# Load BERT model
@st.cache_resource
def load_bert_model():
    try:
        # Th·ª≠ t·∫£i PhoBERT (m√¥ h√¨nh ti·∫øng Vi·ªát)
        tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base", local_files_only=False)
        model = AutoModel.from_pretrained("vinai/phobert-base", local_files_only=False)
        return tokenizer, model, "PhoBERT"
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫£i PhoBERT: {str(e)}")
        st.info("üîÑ ƒêang chuy·ªÉn sang s·ª≠ d·ª•ng m√¥ h√¨nh multilingual-MiniLM...")
        try:
            # Fallback: S·ª≠ d·ª•ng m√¥ h√¨nh nh·ªè h∆°n, h·ªó tr·ª£ ƒëa ng√¥n ng·ªØ
            tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
            model = AutoModel.from_pretrained("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
            return tokenizer, model, "Multilingual-MiniLM"
        except Exception as e2:
            st.error(f"‚ùå Kh√¥ng th·ªÉ t·∫£i b·∫•t k·ª≥ m√¥ h√¨nh BERT n√†o: {str(e2)}")
            st.info("üí° Vui l√≤ng ki·ªÉm tra k·∫øt n·ªëi internet ho·∫∑c t·∫£i m√¥ h√¨nh th·ªß c√¥ng")
            return None, None, None

def get_bert_embedding(word, tokenizer, model):
    if tokenizer is None or model is None:
        return None
    try:
        inputs = tokenizer(word, return_tensors="pt", padding=True, truncation=True)
        with torch.no_grad():
            outputs = model(**inputs)
        # L·∫•y mean c·ªßa hidden states
        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()
        return embeddings
    except Exception as e:
        st.warning(f"L·ªói khi t√≠nh BERT embedding cho '{word}': {str(e)}")
        return None

def get_glove_embedding(word, model):
    try:
        return model.wv[word]
    except KeyError:
        return None

def compute_similarity(vec1, vec2):
    if vec1 is None or vec2 is None:
        return None
    return 1 - cosine(vec1, vec2)

def classify_relationship(similarity, threshold_syn=0.7, threshold_ant=0.3):
    if similarity is None:
        return "‚ùì Kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c"
    elif similarity > threshold_syn:
        return "‚úÖ T·ª´ ƒë·ªìng nghƒ©a"
    elif similarity < threshold_ant:
        return "‚ùå T·ª´ tr√°i nghƒ©a"
    else:
        return "‚ö™ T·ª´ kh√¥ng li√™n quan"

def plot_vector_map(embeddings_dict, word1, word2, title):
    # L·∫•y t·∫•t c·∫£ vectors
    words = list(embeddings_dict.keys())
    vectors = np.array([embeddings_dict[w] for w in words])
    
    # Gi·∫£m chi·ªÅu xu·ªëng 2D
    pca = PCA(n_components=2)
    vectors_2d = pca.fit_transform(vectors)
    
    # T·∫°o dataframe
    df_plot = pd.DataFrame({
        'x': vectors_2d[:, 0],
        'y': vectors_2d[:, 1],
        'word': words
    })
    
    # X√°c ƒë·ªãnh m√†u s·∫Øc
    colors = []
    sizes = []
    for w in words:
        if w == word1 or w == word2:
            colors.append('red')
            sizes.append(15)
        else:
            colors.append('black')
            sizes.append(8)
    
    # V·∫Ω bi·ªÉu ƒë·ªì
    fig = go.Figure()
    
    # Th√™m c√°c ƒëi·ªÉm
    fig.add_trace(go.Scatter(
        x=df_plot['x'],
        y=df_plot['y'],
        mode='markers+text',
        marker=dict(size=sizes, color=colors, opacity=0.7),
        text=df_plot['word'],
        textposition='top center',
        textfont=dict(size=10),
        hovertemplate='<b>%{text}</b><br>x: %{x:.2f}<br>y: %{y:.2f}<extra></extra>'
    ))
    
    # V·∫Ω ƒë∆∞·ªùng n·ªëi gi·ªØa 2 t·ª´ ki·ªÉm tra
    if word1 in words and word2 in words:
        idx1 = words.index(word1)
        idx2 = words.index(word2)
        fig.add_trace(go.Scatter(
            x=[vectors_2d[idx1, 0], vectors_2d[idx2, 0]],
            y=[vectors_2d[idx1, 1], vectors_2d[idx2, 1]],
            mode='lines',
            line=dict(color='red', width=2, dash='dash'),
            showlegend=False,
            hoverinfo='skip'
        ))
    
    fig.update_layout(
        title=title,
        xaxis_title="PC1",
        yaxis_title="PC2",
        height=500,
        showlegend=False,
        hovermode='closest'
    )
    
    return fig

# Main app
try:
    df = load_data()
    st.success(f"‚úÖ ƒê√£ load {len(df)} c·∫∑p t·ª´ t·ª´ dataset")
    
    # Hi·ªÉn th·ªã th√¥ng tin dataset
    with st.expander("üìä Xem dataset"):
        st.dataframe(df.head(20))
    
    # Train models
    with st.spinner("üîÑ ƒêang hu·∫•n luy·ªán m√¥ h√¨nh GloVe..."):
        glove_model = train_glove_model(df)
    st.success("‚úÖ ƒê√£ hu·∫•n luy·ªán xong m√¥ h√¨nh GloVe")
    
    with st.spinner("üîÑ ƒêang t·∫£i m√¥ h√¨nh BERT..."):
        tokenizer, bert_model, model_name = load_bert_model()
    
    if tokenizer is not None and bert_model is not None:
        st.success(f"‚úÖ ƒê√£ t·∫£i xong m√¥ h√¨nh BERT ({model_name})")
        bert_available = True
    else:
        st.error("‚ùå Kh√¥ng th·ªÉ t·∫£i m√¥ h√¨nh BERT. Ch·ªâ s·ª≠ d·ª•ng GloVe.")
        bert_available = False
    
    # L·∫•y t·∫•t c·∫£ t·ª´ unique
    all_words = pd.concat([df['word1'], df['word2']]).unique().tolist()
    
    # T√≠nh embeddings cho t·∫•t c·∫£ t·ª´
    with st.spinner("üîÑ ƒêang t√≠nh embeddings..."):
        glove_embeddings = {}
        bert_embeddings = {}
        
        progress_bar = st.progress(0)
        total_words = len(all_words)
        
        for idx, word in enumerate(all_words):
            # GloVe embeddings
            glove_emb = get_glove_embedding(word, glove_model)
            if glove_emb is not None:
                glove_embeddings[word] = glove_emb
            
            # BERT embeddings (ch·ªâ khi c√≥ m√¥ h√¨nh)
            if bert_available:
                bert_emb = get_bert_embedding(word, tokenizer, bert_model)
                if bert_emb is not None:
                    bert_embeddings[word] = bert_emb
            
            # C·∫≠p nh·∫≠t progress
            progress_bar.progress((idx + 1) / total_words)
        
        progress_bar.empty()
    
    st.success(f"‚úÖ ƒê√£ t√≠nh embeddings cho {len(all_words)} t·ª´ (GloVe: {len(glove_embeddings)}, BERT: {len(bert_embeddings)})")
    
    # Input section
    st.markdown("---")
    st.markdown("### üîç Ki·ªÉm Tra Hai T·ª´")
    
    col1, col2 = st.columns(2)
    with col1:
        word1 = st.selectbox("Ch·ªçn t·ª´ th·ª© nh·∫•t:", all_words, index=0)
    with col2:
        word2 = st.selectbox("Ch·ªçn t·ª´ th·ª© hai:", all_words, index=1)
    
    if st.button("üöÄ Ph√¢n T√≠ch", type="primary"):
        st.markdown("---")
        
        # GloVe Analysis
        st.markdown("## üìä Ph√¢n T√≠ch v·ªõi GloVe")
        col_g1, col_g2 = st.columns([1, 2])
        
        with col_g1:
            glove_vec1 = glove_embeddings.get(word1)
            glove_vec2 = glove_embeddings.get(word2)
            glove_sim = compute_similarity(glove_vec1, glove_vec2)
            glove_rel = classify_relationship(glove_sim)
            
            st.metric("ƒê·ªô t∆∞∆°ng ƒë·ªìng", f"{glove_sim:.4f}" if glove_sim else "N/A")
            st.markdown(f"### {glove_rel}")
            
            if glove_sim:
                st.progress(glove_sim)
        
        with col_g2:
            if glove_vec1 is not None and glove_vec2 is not None:
                fig_glove = plot_vector_map(glove_embeddings, word1, word2, 
                                           "Vector Map - GloVe")
                st.plotly_chart(fig_glove, use_container_width=True)
            else:
                st.warning("Kh√¥ng th·ªÉ v·∫Ω map cho GloVe (thi·∫øu embeddings)")
        
        st.markdown("---")
        
        # BERT Analysis
        if bert_available and len(bert_embeddings) > 0:
            st.markdown(f"## üìä Ph√¢n T√≠ch v·ªõi BERT ({model_name})")
            col_b1, col_b2 = st.columns([1, 2])
            
            with col_b1:
                bert_vec1 = bert_embeddings.get(word1)
                bert_vec2 = bert_embeddings.get(word2)
                bert_sim = compute_similarity(bert_vec1, bert_vec2)
                bert_rel = classify_relationship(bert_sim)
                
                st.metric("ƒê·ªô t∆∞∆°ng ƒë·ªìng", f"{bert_sim:.4f}" if bert_sim else "N/A")
                st.markdown(f"### {bert_rel}")
                
                if bert_sim:
                    st.progress(bert_sim)
            
            with col_b2:
                if bert_vec1 is not None and bert_vec2 is not None:
                    fig_bert = plot_vector_map(bert_embeddings, word1, word2, 
                                              "Vector Map - BERT")
                    st.plotly_chart(fig_bert, use_container_width=True)
                else:
                    st.warning("Kh√¥ng th·ªÉ v·∫Ω map cho BERT (thi·∫øu embeddings)")
            
            # So s√°nh k·∫øt qu·∫£
            st.markdown("---")
            st.markdown("## üìà So S√°nh K·∫øt Qu·∫£")
            comparison_df = pd.DataFrame({
                'M√¥ h√¨nh': ['GloVe', 'BERT'],
                'ƒê·ªô t∆∞∆°ng ƒë·ªìng': [f"{glove_sim:.4f}" if glove_sim else "N/A", 
                                 f"{bert_sim:.4f}" if bert_sim else "N/A"],
                'Quan h·ªá': [glove_rel, bert_rel]
            })
            st.table(comparison_df)
        else:
            st.warning("‚ö†Ô∏è M√¥ h√¨nh BERT kh√¥ng kh·∫£ d·ª•ng. Ch·ªâ hi·ªÉn th·ªã k·∫øt qu·∫£ GloVe.")

except FileNotFoundError:
    st.error("‚ùå Kh√¥ng t√¨m th·∫•y file 'Vietnamese_SynAnt.csv'. Vui l√≤ng ƒë·∫£m b·∫£o file c√≥ trong c√πng th∆∞ m·ª•c.")
except Exception as e:
    st.error(f"‚ùå L·ªói: {str(e)}")
    st.info("H√£y ch·∫Øc ch·∫Øn b·∫°n ƒë√£ c√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt: streamlit, pandas, numpy, scikit-learn, plotly, gensim, transformers, torch")